# ğŸ§  Chatbot Arena and Prompt-to-Leaderboard (P2L): A Comprehensive Technical Review

**Author:** Jiaxin Zhang  
*Last Updated: June 2025*

---

## ğŸ“š Table of Contents

- [ğŸ“Œ Overview](#-overview)
- [ğŸ¥Š Part I: Chatbot Arena â€“ Evaluating with Human Preferences](#-part-i-chatbot-arena--evaluating-with-human-preferences)
  - [ğŸ¯ What is Chatbot Arena?](#-what-is-chatbot-arena)
  - [ğŸ”§ Evaluation Workflow](#-evaluation-workflow)
  - [ğŸ“Š Data Snapshot](#-data-snapshot)
  - [ğŸ“ Bradleyâ€“Terry Model](#-bradleyterry-model)
  - [ğŸ†š BT vs Elo](#-bt-vs-elo)
  - [ğŸ”® Future Directions](#-future-directions)

- [ğŸ§¾ Part II: Prompt-to-Leaderboard (P2L) â€“ Personalized Rankings](#-part-ii-prompt-to-leaderboard-p2l--personalized-rankings)
  - [ğŸ¯ Importance of P2L](#-importance-of-p2l)
  - [ğŸ§± Architecture Overview](#-architecture-overview)
  - [ğŸ” Pipeline Construction](#-pipeline-construction)
  - [ğŸ“¤ Routing Decision Logic](#-routing-decision-logic)
  - [ğŸ“ Results Summary](#-results-summary)
  - [ğŸ“Š P2L Evaluation Metrics](#-p2l-evaluation-metrics)
  - [ğŸ’¡ Applications](#-applications)

- [ğŸ”— Additional Resources](#-additional-resources)
  
---

## ğŸ“Œ Overview

Large Language Models (LLMs), such as GPT-4, Claude, and Gemini, are rapidly advancing. However, evaluating them with human-aligned and scalable frameworks remains an open challenge. LMSYS addresses this with:

- **Chatbot Arena** â€“ A crowdsourced preference-based evaluation platform.
- **Prompt-to-Leaderboard (P2L)** â€“ A system to route prompts to the best model and generate personalized leaderboards.

---

## ğŸ¥Š Part I: Chatbot Arena â€“ Evaluating with Human Preferences

### ğŸ¯ What is Chatbot Arena?

Chatbot Arena is a **scalable, model-agnostic, and preference-based evaluation** system. It crowdsources real human judgments via anonymous model pair comparisons, simulating real-world user experience.

---

### ğŸ”§ Evaluation Workflow

1. Users submit a prompt.
2. Two models (Model A and B) generate answers anonymously.
3. Users compare responses and vote for the better one.
4. Models are shuffled to avoid bias.
5. Votes are logged for statistical ranking.

---

### ğŸ“Š Data Snapshot

| Metric           | Value        |
|------------------|--------------|
| Total Votes      | 2.8M+        |
| Unique Prompts   | 300,000+     |
| Models Compared  | 219+         |
| Domains          | Math, Code, Dialogue, Reasoning, Writing |

---

### ğŸ“ Bradleyâ€“Terry Model

Arena uses the **Bradleyâ€“Terry (BT)** model to infer global rankings from pairwise human preferences.

#### Probability a model \( i \) wins over model \( j \):

```
P(i > j) = 1 / (1 + exp(Î¾_j - Î¾_i))
```

Where \( Î¾_i \) is the skill score of model \( i \).

#### Likelihood Function:

```
L(Î¾) = Î£_{iâ‰ j} n_ij * log(1 / (1 + exp(Î¾_j - Î¾_i)))
```

- \( n_{ij} \): number of times \( i \) beats \( j \)
- Solved via Maximum Likelihood Estimation (MLE)

---

### ğŸ†š BT vs Elo

| Feature             | Bradleyâ€“Terry (BT)     | Elo                      |
|---------------------|-------------------------|---------------------------|
| Data Requirements   | Sparse, asymmetric      | Repeated symmetric games |
| Stability           | High (MLE-based)        | Medium (Online updates)  |
| Transitive Inference| Yes                     | Limited                  |
| Use Case Fit        | Arena (LLM outputs)     | Chess, Go, etc.          |

**Bottom line**: BT better captures LLM evaluation complexities.

---



## ğŸ”® Future Directions

As the Arena and P2L systems mature, several promising directions are emerging to improve personalization, fairness, and adaptability:

- **Cluster-Aware BT Models:**  
  Adapt the Bradleyâ€“Terry model to account for clusters of users with distinct preferences, allowing more nuanced inference.

- **User-Customized Leaderboards:**  
  Generate leaderboard views tailored to individual or grouped user behavior, preferences, and task domains.

- **Preference-Aligned Deployments:**  
  Deploy LLMs that dynamically adapt to user intent â€” factual, creative, empathetic â€” by routing requests to the most aligned model.

These innovations pave the way for truly user-centric and interpretable AI evaluation.


## ğŸ§¾ Part II: Prompt-to-Leaderboard (P2L) â€“ Personalized Rankings

### ğŸ¯ Importance of P2L

Different models shine in different areas:

- GPT-4: Mathematics
- Claude-3: Logical Reasoning
- Gemini: Multimodal tasks

**Goal**: Route prompts to the optimal model and build dynamic rankings.

---

### ğŸ§± Architecture Overview

**Input**: Prompt \( Z \), Model Encoding \( X \) (-1 for A, +1 for B)\
**Output**: Preference label \( Y \in \{0, 1\} \)

- Use LoRA-tuned LLMs (e.g., Qwen2.5-1.5B)
- Train to predict preference probability via sigmoid head

```
Å· = Ïƒ(Xáµ€ Î¸Ì‚(Z))
```

---

### ğŸ” Pipeline Construction

#### 1. Data

- Arena-55K samples
- Format: (Prompt, Model A, Model B, Vote)

#### 2. Training

- Binary Cross-Entropy Loss:
```
L = - y * log(Å·) - (1 - y) * log(1 - Å·)
```

- Batch size: 4
- Max seq length: 4096
- Optimizer: Adam
- LR: 1e-5 ~ 5e-5

#### 3. Inference

- For prompt \( Z \), predict preference between any two models \( (i, j) \)
- Construct matrix of pairwise win probabilities

---

### ğŸ“¤ Routing Decision Logic

- Compute optimal mixed strategy \( \pi^* \in \Delta_M \)

```
Ï€* = argmax_Ï€ Î£_{i,j} Ï€_i * P(i > j) * q_j
```

Where:
- \( Ï€ \): model deployment probabilities
- \( q_j \): weights for model \( j \)

**Algorithms**:
- Borda Count
- Expected Rank Minimization
- Plackettâ€“Luce Sampling

---

### ğŸ“Š P2L Evaluation Metrics

- **Local Accuracy**: Binary correctness on pairwise vote prediction
- **Log Loss**: Model confidence calibration
- **Top-k Precision**: Ranking accuracy at leaderboard head
- **Kendallâ€™s Ï„ / Spearmanâ€™s Ï**: Rank correlation with ground truth
- **Spread**: Variation in P2L score across prompts (used to filter ambiguous queries)

---

### ğŸ“ Results Summary

| Task Domain        | Best Individual Model | P2L Routed Accuracy |
|--------------------|------------------------|----------------------|
| General Tasks      | GPT-4                  | 76.7%                |
| Mathematics        | GPT-4                  | 78.3%                |
| Programming        | Claude-3               | 81.2%                |
| Logical Reasoning  | GPT-4                  | 75.5%                |

---

### ğŸ’¡ Applications

Prompt-level evaluation enables fine-grained control, diagnostic power, and personalization across LLM workflows. Below are major application areas:

- ğŸ” **Optimal Routing**  
  Use per-prompt scores to automatically select the best model for each input.  
  - Dynamically dispatch prompts based on past performance.  
  - Power ensemble systems where different models specialize in different tasks.  
  - Improve multi-agent frameworks and chat systems through adaptive routing.  

- ğŸ‘¤ **Personalized Evaluation**  
  Build custom rankings based on individual user preferences and behaviors.  
  - Track user-specific feedback across prompt types.  
  - Enable user profiles that evolve with usage.  
  - Prioritize models that align with a userâ€™s domain (e.g., legal, technical, creative).  

- ğŸ©º **Automated Diagnosis**  
  Analyze model performance on specific prompt types or domains to find where it struggles (blind spots) and where it excels (strengths). 
  - Detect patterns of failure (e.g., hallucination, inconsistency).  
  - Visualize model performance heatmaps across topics or difficulty.  
  - Inform targeted fine-tuning and data augmentation strategies.  
---

## ğŸ”— Additional Resources

- ğŸ“˜ [Chatbot Arena Paper](https://arxiv.org/abs/2403.04132)
- ğŸ“˜ [P2L Paper](https://arxiv.org/abs/2502.14855)
- ğŸ§  [Chatbot Arena Website](https://chat.lmsys.org/)
- ğŸ“‚ [P2L GitHub](https://github.com/lmarena/p2l)
- ğŸ“‘ [P2L Slides](./p2l_slides.pdf)

